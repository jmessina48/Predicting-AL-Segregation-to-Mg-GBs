{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "65947a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import scipy.io as io\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "import pickle \n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math as m\n",
    "\n",
    "py.init_notebook_mode(connected=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "55f3c3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading Matlab Struct array with all of the data:\n",
    "\n",
    "mat = io.loadmat('data_Mg_GBperatom_seg_2Al_dump.mat')\n",
    "\n",
    "length_A = mat['A'].shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45d2f8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organizing data:\n",
    "\n",
    "for i in range(30):\n",
    "    segE = mat['A']['Eseg'][0,i]\n",
    "    #check whether this is a valid data?\n",
    "    n1 = segE[:,0] != 0 \n",
    "    segE = np.squeeze(segE[n1,:])\n",
    "    atom_ID = segE[:,0].astype(int) - 1\n",
    "\n",
    "    descriptor = mat['A']['peratom'][0,i][0,0]\n",
    "    descriptor_temp = np.concatenate([descriptor['pos'],descriptor['pe'],descriptor['cna'],descriptor['centro_fnn'],\n",
    "                                descriptor['centro_snn'],descriptor['coord'],descriptor['f'],descriptor['stress'],\n",
    "                                descriptor['voronoi']], axis = 1)\n",
    "    if i == 0:\n",
    "        descriptor_all = descriptor_temp[atom_ID]\n",
    "        segE_all = segE\n",
    "    else:\n",
    "        descriptor_temp = descriptor_temp[atom_ID]\n",
    "        descriptor_all = np.concatenate([descriptor_all, descriptor_temp], axis = 0)\n",
    "        segE_all = np.concatenate([segE_all, segE])\n",
    "\n",
    "descriptor_all[:,2] = abs(descriptor_all[:,2]-min(descriptor_all[:,2])-20)\n",
    "sigma_H = np.sum(descriptor_all[:,11:14], axis = 1)/3\n",
    "f_mag = np.linalg.norm(descriptor_all[:,8:11], axis = 1, ord = 2)\n",
    "\n",
    "feature = np.concatenate([descriptor_all, sigma_H[:,np.newaxis], f_mag[:,np.newaxis]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e264a3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2847, 15) \n",
      "\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2847,) \n",
      "\n",
      "----------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "y_true = segE_all[:,1]    # y_true= MD segregation energies\n",
    "\n",
    "pos = feature[:,:3]\n",
    "zpos = pos[:,2][:,np.newaxis]\n",
    "ypos= pos[:,1][:,np.newaxis]\n",
    "xpos= pos[:,0][:,np.newaxis]\n",
    "\n",
    "feature1 = feature[:,3:]\n",
    "\n",
    "# Separation of each descriptor (feature):\n",
    "f0= feature1[:,0][:,np.newaxis]\n",
    "f1= feature1[:,1][:,np.newaxis]\n",
    "f2= feature1[:,2][:,np.newaxis]\n",
    "f3= feature1[:,3][:,np.newaxis]\n",
    "f4= feature1[:,4][:,np.newaxis]\n",
    "f5= feature1[:,5][:,np.newaxis]\n",
    "f6= feature1[:,6][:,np.newaxis]\n",
    "f7= feature1[:,7][:,np.newaxis]\n",
    "f8= feature1[:,8][:,np.newaxis]\n",
    "f9= feature1[:,9][:,np.newaxis]\n",
    "f10= feature1[:,10][:,np.newaxis]\n",
    "f11= feature1[:,11][:,np.newaxis]\n",
    "f12= feature1[:,12][:,np.newaxis]\n",
    "f13= feature1[:,13][:,np.newaxis]\n",
    "f14= feature1[:,14][:,np.newaxis]\n",
    "f15= feature1[:,15][:,np.newaxis]\n",
    "f16= feature1[:,16][:,np.newaxis]\n",
    "f17= feature1[:,17][:,np.newaxis]\n",
    "\n",
    "# Removal of least important features below \n",
    "# The optimal combination of features to use was found using \n",
    "# Recursive Feature Elimination. This was applied\n",
    "# to each base estimator (see bottom of file for more info):\n",
    "feature1 = np.concatenate([f0,f1,f2,f3,f4,f8,f9,f10,f11,f13,f14,f15,f16,xpos,ypos], axis=1)\n",
    "\n",
    "n= feature1.shape[1]\n",
    "feature_space = feature1\n",
    "\n",
    "\n",
    "#random shuffle\n",
    "np.random.seed(10)   #Randomizing Seed\n",
    "idx0 = np.random.permutation(np.arange(len(feature_space)))  \n",
    "feature_space = feature_space[idx0]\n",
    "y_true = y_true[idx0]\n",
    "\n",
    "# #divide data into training, crossvalildation and test sets:\n",
    "# X_train = feature_space[0:int(len(feature_space)*.7)]\n",
    "# X_mean = np.mean(X_train, axis = 0)\n",
    "# X_std = np.std(X_train, axis = 0)\n",
    "# X_train = (X_train - X_mean)/X_std\n",
    "# X_temp = feature_space[int(len(feature_space)*.7):]\n",
    "# X_temp = (X_temp - X_mean)/X_std\n",
    "# X_test = X_temp[:int(len(X_temp)*.5)]\n",
    "# X_cos = X_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "# Y_train = y_true[0:int(len(feature_space)*.7)]\n",
    "# Y_mean = np.mean(Y_train, axis = 0)\n",
    "# Y_std = np.std(Y_train, axis = 0)\n",
    "# Y_temp = y_true[int(len(feature_space)*.7):]\n",
    "# Y_test = Y_temp[:int(len(X_temp)*.5)]\n",
    "# Y_cos = Y_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_fold1 = feature_space[0:int(len(feature_space)*.2)]\n",
    "X_fold2 = feature_space[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "X_fold3 = feature_space[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "X_fold4 = feature_space[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "X_fold5 = feature_space[int(len(feature_space)*.8):]\n",
    "\n",
    "X_std1 = np.std(X_fold1, axis = 0)\n",
    "X_std2 = np.std(X_fold2, axis = 0)\n",
    "X_std3 = np.std(X_fold3, axis = 0)\n",
    "X_std4 = np.std(X_fold4, axis = 0)\n",
    "X_std5 = np.std(X_fold5, axis = 0)\n",
    "\n",
    "X_mean1 = np.mean(X_fold1, axis = 0)\n",
    "X_mean2 = np.mean(X_fold2, axis = 0)\n",
    "X_mean3 = np.mean(X_fold3, axis = 0)\n",
    "X_mean4 = np.mean(X_fold4, axis = 0)\n",
    "X_mean5 = np.mean(X_fold5, axis = 0)\n",
    "\n",
    "X_fold1 = (X_fold1 - X_mean1)/X_std1\n",
    "X_fold2 = (X_fold2 - X_mean2)/X_std2\n",
    "X_fold3 = (X_fold3 - X_mean3)/X_std3\n",
    "X_fold4 = (X_fold4 - X_mean4)/X_std4\n",
    "X_fold5 = (X_fold5 - X_mean5)/X_std5\n",
    "\n",
    "\n",
    "y_fold1 = y_true[0:int(len(feature_space)*.2)]\n",
    "y_fold2 = y_true[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "y_fold3 = y_true[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "y_fold4 = y_true[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "y_fold5 = y_true[int(len(feature_space)*.8):]\n",
    "\n",
    "\n",
    "\n",
    "print(X_fold1.shape)\n",
    "print(X_fold2.shape)\n",
    "print(X_fold3.shape)\n",
    "print(X_fold4.shape)\n",
    "print(X_fold5.shape,'\\n')\n",
    "\n",
    "print(y_fold1.shape)\n",
    "print(y_fold2.shape)\n",
    "print(y_fold3.shape)\n",
    "print(y_fold4.shape)\n",
    "print(y_fold5.shape, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "print('-----------------------------','\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b070d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "R2_train_list =[]\n",
    "R2_val_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "02db38d2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0 64.57832336425781\n",
      "1000 0.4667019844055176\n",
      "2000 0.44009822607040405\n",
      "3000 0.4143255949020386\n",
      "4000 0.3887212574481964\n",
      "5000 0.34665828943252563\n",
      "6000 0.3366284966468811\n",
      "7000 0.31479859352111816\n",
      "8000 0.3381136953830719\n",
      "9000 0.3244890868663788\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "\n",
    "(N, D_in) = X_train1.shape\n",
    "H1 = 200\n",
    "H2 = 50\n",
    "H3 = 25\n",
    "D_out = 1\n",
    "\n",
    "print(D_in)\n",
    "#print(H)\n",
    "\n",
    "X_train1 = torch.from_numpy(X_train1)\n",
    "X_train1 = X_train1.float()\n",
    "y = torch.from_numpy(y_train1)\n",
    "y = y.float()\n",
    "y = y.view(y.shape[0],1)\n",
    "modelnnbcv1 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H3, D_out)\n",
    ")\n",
    "\n",
    "#model1.apply(weights_init)     # initialize weights\n",
    "\n",
    "loss_fn2 = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4    #orig: 1e-4\n",
    "\n",
    "#optimizer2 = torch.optim.SGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.Adagrad(modelnn.parameters(), lr= learning_rate, lr_decay= 0.1, weight_decay=0.3, initial_accumulator_value=0)#, eps= 1e-10)\n",
    "optimizer2= torch.optim.RMSprop(modelnnbcv1.parameters(), lr = learning_rate, alpha=0.85, eps=1e-08, weight_decay=0.06, momentum=0.9)\n",
    "#optimizer2 = torch.optim.Adamax(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, betas=(0.4, 0.999), eps=1e-6)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.ASGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, lambd=5e-1, alpha=0.65, t0=1e7)#, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "\n",
    "\n",
    "for t in range(10000):\n",
    "    optimizer2.zero_grad()\n",
    "    \n",
    "    y_pred = modelnnbcv1(X_train1)\n",
    "    \n",
    "    loss = loss_fn2(y_pred, y)\n",
    "    if t%1000 == 0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer2.step()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e42ac8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared Train: 0.9849871183215958\n",
      "R_squared val: 0.9643650124224278\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "X_train1 = torch.from_numpy(X_train1)\n",
    "X_train1 = X_train1.float()\n",
    "Y_train_pred = modelnnbcv1(X_train1)   #\n",
    "Y_train_pred = Y_train_pred.detach().numpy()\n",
    "Y_train_pred = Y_train_pred.squeeze()\n",
    "Y_train = y_train1.squeeze()\n",
    "Y_mean = np.mean(Y_train)\n",
    "SS_tot = np.sum(np.power(Y_train - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_train_pred - Y_train, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared Train:', R_squared)\n",
    "R2_train_list.append(R_squared)\n",
    "\n",
    "\n",
    "\n",
    "X_test1 = torch.from_numpy(X_test1)\n",
    "X_test1 = X_test1.float()\n",
    "Y_test_pred = modelnnbcv1(X_test1)    #\n",
    "Y_test_pred = Y_test_pred.detach().numpy()\n",
    "Y_test_pred = Y_test_pred.squeeze()\n",
    "Y_test = y_test1.squeeze()\n",
    "Y_mean = np.mean(Y_test)\n",
    "SS_tot = np.sum(np.power(Y_test - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_test_pred - Y_test, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared val:', R_squared)\n",
    "R2_val_list.append(R_squared)\n",
    "\n",
    "\n",
    "# X_cos = torch.from_numpy(X_cos)\n",
    "# X_cos = X_cos.float()\n",
    "# Y_cos_pred = modelnnb(X_cos)\n",
    "# Y_cos_pred = Y_cos_pred.detach().numpy()\n",
    "# Y_cos_pred = Y_cos_pred.squeeze()\n",
    "# Y_cos = Y_cos.squeeze()\n",
    "# Y_mean = np.mean(Y_cos)\n",
    "# SS_tot = np.sum(np.power(Y_cos - Y_mean, 2))\n",
    "# SS_res = np.sum(np.power(Y_cos_pred - Y_cos, 2))\n",
    "# R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "# print('R_squared Cross Validation:', R_squared)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af91ec53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.009009650532740512 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation\n",
    "Y_test_pred_nn = modelnnbcv1(X_test1)\n",
    "Y_test_pred_nn = Y_test_pred_nn.detach().numpy()\n",
    "Y_test_pred_nn = Y_test_pred_nn.squeeze()\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test1,Y_test_pred_nn)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2814a020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "modelnnbcv1_pkl_filename = 'modelnnbcv1.pkl'\n",
    "modelnnbcv1_pkl = open(modelnnbcv1_pkl_filename, 'wb')\n",
    "pickle.dump(modelnnbcv1, modelnnbcv1_pkl)\n",
    "modelnnbcv1_pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8257a",
   "metadata": {},
   "source": [
    "# Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "68c58da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2847, 15) \n",
      "\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2847,) \n",
      "\n",
      "----------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "y_true = segE_all[:,1]    # y_true= MD segregation energies\n",
    "\n",
    "pos = feature[:,:3]\n",
    "zpos = pos[:,2][:,np.newaxis]\n",
    "ypos= pos[:,1][:,np.newaxis]\n",
    "xpos= pos[:,0][:,np.newaxis]\n",
    "\n",
    "feature1 = feature[:,3:]\n",
    "\n",
    "# Separation of each descriptor (feature):\n",
    "f0= feature1[:,0][:,np.newaxis]\n",
    "f1= feature1[:,1][:,np.newaxis]\n",
    "f2= feature1[:,2][:,np.newaxis]\n",
    "f3= feature1[:,3][:,np.newaxis]\n",
    "f4= feature1[:,4][:,np.newaxis]\n",
    "f5= feature1[:,5][:,np.newaxis]\n",
    "f6= feature1[:,6][:,np.newaxis]\n",
    "f7= feature1[:,7][:,np.newaxis]\n",
    "f8= feature1[:,8][:,np.newaxis]\n",
    "f9= feature1[:,9][:,np.newaxis]\n",
    "f10= feature1[:,10][:,np.newaxis]\n",
    "f11= feature1[:,11][:,np.newaxis]\n",
    "f12= feature1[:,12][:,np.newaxis]\n",
    "f13= feature1[:,13][:,np.newaxis]\n",
    "f14= feature1[:,14][:,np.newaxis]\n",
    "f15= feature1[:,15][:,np.newaxis]\n",
    "f16= feature1[:,16][:,np.newaxis]\n",
    "f17= feature1[:,17][:,np.newaxis]\n",
    "\n",
    "# Removal of least important features below \n",
    "# The optimal combination of features to use was found using \n",
    "# Recursive Feature Elimination. This was applied\n",
    "# to each base estimator (see bottom of file for more info):\n",
    "feature1 = np.concatenate([f0,f1,f2,f3,f4,f8,f9,f10,f11,f13,f14,f15,f16,xpos,ypos], axis=1)\n",
    "\n",
    "n= feature1.shape[1]\n",
    "feature_space = feature1\n",
    "\n",
    "\n",
    "#random shuffle\n",
    "np.random.seed(10)   #Randomizing Seed\n",
    "idx0 = np.random.permutation(np.arange(len(feature_space)))  \n",
    "feature_space = feature_space[idx0]\n",
    "y_true = y_true[idx0]\n",
    "\n",
    "# #divide data into training, crossvalildation and test sets:\n",
    "# X_train = feature_space[0:int(len(feature_space)*.7)]\n",
    "# X_mean = np.mean(X_train, axis = 0)\n",
    "# X_std = np.std(X_train, axis = 0)\n",
    "# X_train = (X_train - X_mean)/X_std\n",
    "# X_temp = feature_space[int(len(feature_space)*.7):]\n",
    "# X_temp = (X_temp - X_mean)/X_std\n",
    "# X_test = X_temp[:int(len(X_temp)*.5)]\n",
    "# X_cos = X_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "# Y_train = y_true[0:int(len(feature_space)*.7)]\n",
    "# Y_mean = np.mean(Y_train, axis = 0)\n",
    "# Y_std = np.std(Y_train, axis = 0)\n",
    "# Y_temp = y_true[int(len(feature_space)*.7):]\n",
    "# Y_test = Y_temp[:int(len(X_temp)*.5)]\n",
    "# Y_cos = Y_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_fold1 = feature_space[0:int(len(feature_space)*.2)]\n",
    "X_fold2 = feature_space[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "X_fold3 = feature_space[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "X_fold4 = feature_space[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "X_fold5 = feature_space[int(len(feature_space)*.8):]\n",
    "\n",
    "X_std1 = np.std(X_fold1, axis = 0)\n",
    "X_std2 = np.std(X_fold2, axis = 0)\n",
    "X_std3 = np.std(X_fold3, axis = 0)\n",
    "X_std4 = np.std(X_fold4, axis = 0)\n",
    "X_std5 = np.std(X_fold5, axis = 0)\n",
    "\n",
    "X_mean1 = np.mean(X_fold1, axis = 0)\n",
    "X_mean2 = np.mean(X_fold2, axis = 0)\n",
    "X_mean3 = np.mean(X_fold3, axis = 0)\n",
    "X_mean4 = np.mean(X_fold4, axis = 0)\n",
    "X_mean5 = np.mean(X_fold5, axis = 0)\n",
    "\n",
    "X_fold1 = (X_fold1 - X_mean1)/X_std1\n",
    "X_fold2 = (X_fold2 - X_mean2)/X_std2\n",
    "X_fold3 = (X_fold3 - X_mean3)/X_std3\n",
    "X_fold4 = (X_fold4 - X_mean4)/X_std4\n",
    "X_fold5 = (X_fold5 - X_mean5)/X_std5\n",
    "\n",
    "\n",
    "y_fold1 = y_true[0:int(len(feature_space)*.2)]\n",
    "y_fold2 = y_true[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "y_fold3 = y_true[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "y_fold4 = y_true[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "y_fold5 = y_true[int(len(feature_space)*.8):]\n",
    "\n",
    "\n",
    "\n",
    "print(X_fold1.shape)\n",
    "print(X_fold2.shape)\n",
    "print(X_fold3.shape)\n",
    "print(X_fold4.shape)\n",
    "print(X_fold5.shape,'\\n')\n",
    "\n",
    "print(y_fold1.shape)\n",
    "print(y_fold2.shape)\n",
    "print(y_fold3.shape)\n",
    "print(y_fold4.shape)\n",
    "print(y_fold5.shape, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "print('-----------------------------','\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "152dd691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0 65.67202758789062\n",
      "1000 0.479742169380188\n",
      "2000 0.41044101119041443\n",
      "3000 0.43867549300193787\n",
      "4000 0.35980623960494995\n",
      "5000 0.40635350346565247\n",
      "6000 0.3795199990272522\n",
      "7000 0.34898841381073\n",
      "8000 0.36741122603416443\n",
      "9000 0.35851922631263733\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "\n",
    "(N, D_in) = X_train2.shape  #\n",
    "H1 = 200\n",
    "H2 = 50\n",
    "H3 = 25\n",
    "D_out = 1\n",
    "\n",
    "print(D_in)\n",
    "#print(H)\n",
    "\n",
    "X_train2 = torch.from_numpy(X_train2)  #\n",
    "X_train2 = X_train2.float()            #\n",
    "y = torch.from_numpy(y_train2)         #\n",
    "y = y.float()\n",
    "y = y.view(y.shape[0],1)\n",
    "modelnnbcv2 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H3, D_out)\n",
    ")\n",
    "\n",
    "#model1.apply(weights_init)     # initialize weights\n",
    "\n",
    "loss_fn2 = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4    #orig: 1e-4\n",
    "\n",
    "#optimizer2 = torch.optim.SGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.Adagrad(modelnn.parameters(), lr= learning_rate, lr_decay= 0.1, weight_decay=0.3, initial_accumulator_value=0)#, eps= 1e-10)\n",
    "optimizer2= torch.optim.RMSprop(modelnnbcv2.parameters(), lr = learning_rate, alpha=0.85, eps=1e-08, weight_decay=0.06, momentum=0.9)\n",
    "#optimizer2 = torch.optim.Adamax(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, betas=(0.4, 0.999), eps=1e-6)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.ASGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, lambd=5e-1, alpha=0.65, t0=1e7)#, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "\n",
    "\n",
    "for t in range(10000):\n",
    "    optimizer2.zero_grad()\n",
    "    \n",
    "    y_pred = modelnnbcv2(X_train2)     #\n",
    "    \n",
    "    loss = loss_fn2(y_pred, y)\n",
    "    if t%1000 == 0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer2.step()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2522ee7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared Train: 0.9855009500880881\n",
      "R_squared val: 0.9621413513406336\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train2 = torch.from_numpy(X_train2)        #\n",
    "X_train2 = X_train2.float()                 #\n",
    "Y_train_pred = modelnnbcv2(X_train2)           #\n",
    "Y_train_pred = Y_train_pred.detach().numpy()\n",
    "Y_train_pred = Y_train_pred.squeeze()\n",
    "Y_train = y_train2.squeeze()                #\n",
    "Y_mean = np.mean(Y_train)\n",
    "SS_tot = np.sum(np.power(Y_train - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_train_pred - Y_train, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared Train:', R_squared)\n",
    "R2_train_list.append(R_squared)\n",
    "\n",
    "\n",
    "X_test2 = torch.from_numpy(X_test2)      #\n",
    "X_test2 = X_test2.float()                #\n",
    "Y_test_pred = modelnnbcv2(X_test2)         #\n",
    "Y_test_pred = Y_test_pred.detach().numpy()\n",
    "Y_test_pred = Y_test_pred.squeeze()\n",
    "Y_test = y_test2.squeeze()            #\n",
    "Y_mean = np.mean(Y_test)\n",
    "SS_tot = np.sum(np.power(Y_test - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_test_pred - Y_test, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared val:', R_squared)\n",
    "R2_val_list.append(R_squared)\n",
    "\n",
    "\n",
    "# X_cos = torch.from_numpy(X_cos)\n",
    "# X_cos = X_cos.float()\n",
    "# Y_cos_pred = modelnnb(X_cos)\n",
    "# Y_cos_pred = Y_cos_pred.detach().numpy()\n",
    "# Y_cos_pred = Y_cos_pred.squeeze()\n",
    "# Y_cos = Y_cos.squeeze()\n",
    "# Y_mean = np.mean(Y_cos)\n",
    "# SS_tot = np.sum(np.power(Y_cos - Y_mean, 2))\n",
    "# SS_res = np.sum(np.power(Y_cos_pred - Y_cos, 2))\n",
    "# R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "# print('R_squared Cross Validation:', R_squared)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6c478fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "modelnnbcv2_pkl_filename = 'modelnnbcv2.pkl'\n",
    "modelnnbcv2_pkl = open(modelnnbcv2_pkl_filename, 'wb')\n",
    "pickle.dump(modelnnbcv2, modelnnbcv2_pkl)\n",
    "modelnnbcv2_pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f901d0",
   "metadata": {},
   "source": [
    "# Round 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa6f2747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2847, 15) \n",
      "\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2847,) \n",
      "\n",
      "----------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "y_true = segE_all[:,1]    # y_true= MD segregation energies\n",
    "\n",
    "pos = feature[:,:3]\n",
    "zpos = pos[:,2][:,np.newaxis]\n",
    "ypos= pos[:,1][:,np.newaxis]\n",
    "xpos= pos[:,0][:,np.newaxis]\n",
    "\n",
    "feature1 = feature[:,3:]\n",
    "\n",
    "# Separation of each descriptor (feature):\n",
    "f0= feature1[:,0][:,np.newaxis]\n",
    "f1= feature1[:,1][:,np.newaxis]\n",
    "f2= feature1[:,2][:,np.newaxis]\n",
    "f3= feature1[:,3][:,np.newaxis]\n",
    "f4= feature1[:,4][:,np.newaxis]\n",
    "f5= feature1[:,5][:,np.newaxis]\n",
    "f6= feature1[:,6][:,np.newaxis]\n",
    "f7= feature1[:,7][:,np.newaxis]\n",
    "f8= feature1[:,8][:,np.newaxis]\n",
    "f9= feature1[:,9][:,np.newaxis]\n",
    "f10= feature1[:,10][:,np.newaxis]\n",
    "f11= feature1[:,11][:,np.newaxis]\n",
    "f12= feature1[:,12][:,np.newaxis]\n",
    "f13= feature1[:,13][:,np.newaxis]\n",
    "f14= feature1[:,14][:,np.newaxis]\n",
    "f15= feature1[:,15][:,np.newaxis]\n",
    "f16= feature1[:,16][:,np.newaxis]\n",
    "f17= feature1[:,17][:,np.newaxis]\n",
    "\n",
    "# Removal of least important features below \n",
    "# The optimal combination of features to use was found using \n",
    "# Recursive Feature Elimination. This was applied\n",
    "# to each base estimator (see bottom of file for more info):\n",
    "feature1 = np.concatenate([f0,f1,f2,f3,f4,f8,f9,f10,f11,f13,f14,f15,f16,xpos,ypos], axis=1)\n",
    "\n",
    "n= feature1.shape[1]\n",
    "feature_space = feature1\n",
    "\n",
    "\n",
    "#random shuffle\n",
    "np.random.seed(10)   #Randomizing Seed\n",
    "idx0 = np.random.permutation(np.arange(len(feature_space)))  \n",
    "feature_space = feature_space[idx0]\n",
    "y_true = y_true[idx0]\n",
    "\n",
    "# #divide data into training, crossvalildation and test sets:\n",
    "# X_train = feature_space[0:int(len(feature_space)*.7)]\n",
    "# X_mean = np.mean(X_train, axis = 0)\n",
    "# X_std = np.std(X_train, axis = 0)\n",
    "# X_train = (X_train - X_mean)/X_std\n",
    "# X_temp = feature_space[int(len(feature_space)*.7):]\n",
    "# X_temp = (X_temp - X_mean)/X_std\n",
    "# X_test = X_temp[:int(len(X_temp)*.5)]\n",
    "# X_cos = X_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "# Y_train = y_true[0:int(len(feature_space)*.7)]\n",
    "# Y_mean = np.mean(Y_train, axis = 0)\n",
    "# Y_std = np.std(Y_train, axis = 0)\n",
    "# Y_temp = y_true[int(len(feature_space)*.7):]\n",
    "# Y_test = Y_temp[:int(len(X_temp)*.5)]\n",
    "# Y_cos = Y_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_fold1 = feature_space[0:int(len(feature_space)*.2)]\n",
    "X_fold2 = feature_space[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "X_fold3 = feature_space[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "X_fold4 = feature_space[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "X_fold5 = feature_space[int(len(feature_space)*.8):]\n",
    "\n",
    "X_std1 = np.std(X_fold1, axis = 0)\n",
    "X_std2 = np.std(X_fold2, axis = 0)\n",
    "X_std3 = np.std(X_fold3, axis = 0)\n",
    "X_std4 = np.std(X_fold4, axis = 0)\n",
    "X_std5 = np.std(X_fold5, axis = 0)\n",
    "\n",
    "X_mean1 = np.mean(X_fold1, axis = 0)\n",
    "X_mean2 = np.mean(X_fold2, axis = 0)\n",
    "X_mean3 = np.mean(X_fold3, axis = 0)\n",
    "X_mean4 = np.mean(X_fold4, axis = 0)\n",
    "X_mean5 = np.mean(X_fold5, axis = 0)\n",
    "\n",
    "X_fold1 = (X_fold1 - X_mean1)/X_std1\n",
    "X_fold2 = (X_fold2 - X_mean2)/X_std2\n",
    "X_fold3 = (X_fold3 - X_mean3)/X_std3\n",
    "X_fold4 = (X_fold4 - X_mean4)/X_std4\n",
    "X_fold5 = (X_fold5 - X_mean5)/X_std5\n",
    "\n",
    "\n",
    "y_fold1 = y_true[0:int(len(feature_space)*.2)]\n",
    "y_fold2 = y_true[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "y_fold3 = y_true[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "y_fold4 = y_true[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "y_fold5 = y_true[int(len(feature_space)*.8):]\n",
    "\n",
    "\n",
    "\n",
    "print(X_fold1.shape)\n",
    "print(X_fold2.shape)\n",
    "print(X_fold3.shape)\n",
    "print(X_fold4.shape)\n",
    "print(X_fold5.shape,'\\n')\n",
    "\n",
    "print(y_fold1.shape)\n",
    "print(y_fold2.shape)\n",
    "print(y_fold3.shape)\n",
    "print(y_fold4.shape)\n",
    "print(y_fold5.shape, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "print('-----------------------------','\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2766eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0 65.6483383178711\n",
      "1000 0.4732404351234436\n",
      "2000 0.4563058316707611\n",
      "3000 0.38391849398612976\n",
      "4000 0.3626531660556793\n",
      "5000 0.4006459414958954\n",
      "6000 0.3847166895866394\n",
      "7000 0.3775559365749359\n",
      "8000 0.3329657018184662\n",
      "9000 0.32290950417518616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "\n",
    "(N, D_in) = X_train3.shape  #\n",
    "H1 = 200\n",
    "H2 = 50\n",
    "H3 = 25\n",
    "D_out = 1\n",
    "\n",
    "print(D_in)\n",
    "#print(H)\n",
    "\n",
    "X_train3 = torch.from_numpy(X_train3)  #\n",
    "X_train3 = X_train3.float()            #\n",
    "y = torch.from_numpy(y_train3)         #\n",
    "y = y.float()\n",
    "y = y.view(y.shape[0],1)\n",
    "modelnnbcv3 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H3, D_out)\n",
    ")\n",
    "\n",
    "#model1.apply(weights_init)     # initialize weights\n",
    "\n",
    "loss_fn2 = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4    #orig: 1e-4\n",
    "\n",
    "#optimizer2 = torch.optim.SGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.Adagrad(modelnn.parameters(), lr= learning_rate, lr_decay= 0.1, weight_decay=0.3, initial_accumulator_value=0)#, eps= 1e-10)\n",
    "optimizer2= torch.optim.RMSprop(modelnnbcv3.parameters(), lr = learning_rate, alpha=0.85, eps=1e-08, weight_decay=0.06, momentum=0.9)\n",
    "#optimizer2 = torch.optim.Adamax(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, betas=(0.4, 0.999), eps=1e-6)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.ASGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, lambd=5e-1, alpha=0.65, t0=1e7)#, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "\n",
    "\n",
    "for t in range(10000):\n",
    "    optimizer2.zero_grad()\n",
    "    \n",
    "    y_pred = modelnnbcv3(X_train3)       ##\n",
    "    \n",
    "    loss = loss_fn2(y_pred, y)\n",
    "    if t%1000 == 0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer2.step()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b7465f0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared Train: 0.9834642996058409\n",
      "R_squared val: 0.960232123313809\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "X_train3 = torch.from_numpy(X_train3)        #\n",
    "X_train3 = X_train3.float()                 #\n",
    "Y_train_pred = modelnnbcv3(X_train3)           #\n",
    "Y_train_pred = Y_train_pred.detach().numpy()\n",
    "Y_train_pred = Y_train_pred.squeeze()\n",
    "Y_train = y_train3.squeeze()                #\n",
    "Y_mean = np.mean(Y_train)\n",
    "SS_tot = np.sum(np.power(Y_train - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_train_pred - Y_train, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared Train:', R_squared)\n",
    "R2_train_list.append(R_squared)\n",
    "\n",
    "\n",
    "X_test3 = torch.from_numpy(X_test3)      #\n",
    "X_test3 = X_test3.float()                #\n",
    "Y_test_pred = modelnnbcv3(X_test3)         #\n",
    "Y_test_pred = Y_test_pred.detach().numpy()\n",
    "Y_test_pred = Y_test_pred.squeeze()\n",
    "Y_test = y_test3.squeeze()            #\n",
    "Y_mean = np.mean(Y_test)\n",
    "SS_tot = np.sum(np.power(Y_test - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_test_pred - Y_test, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared val:', R_squared)\n",
    "R2_val_list.append(R_squared)\n",
    "\n",
    "\n",
    "# X_cos = torch.from_numpy(X_cos)\n",
    "# X_cos = X_cos.float()\n",
    "# Y_cos_pred = modelnnb(X_cos)\n",
    "# Y_cos_pred = Y_cos_pred.detach().numpy()\n",
    "# Y_cos_pred = Y_cos_pred.squeeze()\n",
    "# Y_cos = Y_cos.squeeze()\n",
    "# Y_mean = np.mean(Y_cos)\n",
    "# SS_tot = np.sum(np.power(Y_cos - Y_mean, 2))\n",
    "# SS_res = np.sum(np.power(Y_cos_pred - Y_cos, 2))\n",
    "# R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "# print('R_squared Cross Validation:', R_squared)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ef2b69ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving Model\n",
    "\n",
    "# modelnnbcv3_pkl_filename = 'modelnnbcv3.pkl'\n",
    "# modelnnbcv3_pkl = open(modelnnbcv3_pkl_filename, 'wb')\n",
    "# pickle.dump(modelnnbcv3, modelnnbcv3_pkl)\n",
    "# modelnnbcv3_pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a70f49",
   "metadata": {},
   "source": [
    "# Round 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b6d53828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2847, 15) \n",
      "\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2847,) \n",
      "\n",
      "----------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "y_true = segE_all[:,1]    # y_true= MD segregation energies\n",
    "\n",
    "pos = feature[:,:3]\n",
    "zpos = pos[:,2][:,np.newaxis]\n",
    "ypos= pos[:,1][:,np.newaxis]\n",
    "xpos= pos[:,0][:,np.newaxis]\n",
    "\n",
    "feature1 = feature[:,3:]\n",
    "\n",
    "# Separation of each descriptor (feature):\n",
    "f0= feature1[:,0][:,np.newaxis]\n",
    "f1= feature1[:,1][:,np.newaxis]\n",
    "f2= feature1[:,2][:,np.newaxis]\n",
    "f3= feature1[:,3][:,np.newaxis]\n",
    "f4= feature1[:,4][:,np.newaxis]\n",
    "f5= feature1[:,5][:,np.newaxis]\n",
    "f6= feature1[:,6][:,np.newaxis]\n",
    "f7= feature1[:,7][:,np.newaxis]\n",
    "f8= feature1[:,8][:,np.newaxis]\n",
    "f9= feature1[:,9][:,np.newaxis]\n",
    "f10= feature1[:,10][:,np.newaxis]\n",
    "f11= feature1[:,11][:,np.newaxis]\n",
    "f12= feature1[:,12][:,np.newaxis]\n",
    "f13= feature1[:,13][:,np.newaxis]\n",
    "f14= feature1[:,14][:,np.newaxis]\n",
    "f15= feature1[:,15][:,np.newaxis]\n",
    "f16= feature1[:,16][:,np.newaxis]\n",
    "f17= feature1[:,17][:,np.newaxis]\n",
    "\n",
    "# Removal of least important features below \n",
    "# The optimal combination of features to use was found using \n",
    "# Recursive Feature Elimination. This was applied\n",
    "# to each base estimator (see bottom of file for more info):\n",
    "feature1 = np.concatenate([f0,f1,f2,f3,f4,f8,f9,f10,f11,f13,f14,f15,f16,xpos,ypos], axis=1)\n",
    "\n",
    "n= feature1.shape[1]\n",
    "feature_space = feature1\n",
    "\n",
    "\n",
    "#random shuffle\n",
    "np.random.seed(10)   #Randomizing Seed\n",
    "idx0 = np.random.permutation(np.arange(len(feature_space)))  \n",
    "feature_space = feature_space[idx0]\n",
    "y_true = y_true[idx0]\n",
    "\n",
    "# #divide data into training, crossvalildation and test sets:\n",
    "# X_train = feature_space[0:int(len(feature_space)*.7)]\n",
    "# X_mean = np.mean(X_train, axis = 0)\n",
    "# X_std = np.std(X_train, axis = 0)\n",
    "# X_train = (X_train - X_mean)/X_std\n",
    "# X_temp = feature_space[int(len(feature_space)*.7):]\n",
    "# X_temp = (X_temp - X_mean)/X_std\n",
    "# X_test = X_temp[:int(len(X_temp)*.5)]\n",
    "# X_cos = X_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "# Y_train = y_true[0:int(len(feature_space)*.7)]\n",
    "# Y_mean = np.mean(Y_train, axis = 0)\n",
    "# Y_std = np.std(Y_train, axis = 0)\n",
    "# Y_temp = y_true[int(len(feature_space)*.7):]\n",
    "# Y_test = Y_temp[:int(len(X_temp)*.5)]\n",
    "# Y_cos = Y_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_fold1 = feature_space[0:int(len(feature_space)*.2)]\n",
    "X_fold2 = feature_space[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "X_fold3 = feature_space[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "X_fold4 = feature_space[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "X_fold5 = feature_space[int(len(feature_space)*.8):]\n",
    "\n",
    "X_std1 = np.std(X_fold1, axis = 0)\n",
    "X_std2 = np.std(X_fold2, axis = 0)\n",
    "X_std3 = np.std(X_fold3, axis = 0)\n",
    "X_std4 = np.std(X_fold4, axis = 0)\n",
    "X_std5 = np.std(X_fold5, axis = 0)\n",
    "\n",
    "X_mean1 = np.mean(X_fold1, axis = 0)\n",
    "X_mean2 = np.mean(X_fold2, axis = 0)\n",
    "X_mean3 = np.mean(X_fold3, axis = 0)\n",
    "X_mean4 = np.mean(X_fold4, axis = 0)\n",
    "X_mean5 = np.mean(X_fold5, axis = 0)\n",
    "\n",
    "X_fold1 = (X_fold1 - X_mean1)/X_std1\n",
    "X_fold2 = (X_fold2 - X_mean2)/X_std2\n",
    "X_fold3 = (X_fold3 - X_mean3)/X_std3\n",
    "X_fold4 = (X_fold4 - X_mean4)/X_std4\n",
    "X_fold5 = (X_fold5 - X_mean5)/X_std5\n",
    "\n",
    "\n",
    "y_fold1 = y_true[0:int(len(feature_space)*.2)]\n",
    "y_fold2 = y_true[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "y_fold3 = y_true[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "y_fold4 = y_true[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "y_fold5 = y_true[int(len(feature_space)*.8):]\n",
    "\n",
    "\n",
    "\n",
    "print(X_fold1.shape)\n",
    "print(X_fold2.shape)\n",
    "print(X_fold3.shape)\n",
    "print(X_fold4.shape)\n",
    "print(X_fold5.shape,'\\n')\n",
    "\n",
    "print(y_fold1.shape)\n",
    "print(y_fold2.shape)\n",
    "print(y_fold3.shape)\n",
    "print(y_fold4.shape)\n",
    "print(y_fold5.shape, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "print('-----------------------------','\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "98eafec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0 65.95205688476562\n",
      "1000 0.5195485353469849\n",
      "2000 0.4277627170085907\n",
      "3000 0.4133763313293457\n",
      "4000 0.3999062180519104\n",
      "5000 0.419999361038208\n",
      "6000 0.38540974259376526\n",
      "7000 0.3319950997829437\n",
      "8000 0.3407895565032959\n",
      "9000 0.3234460651874542\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "\n",
    "(N, D_in) = X_train4.shape  #\n",
    "H1 = 200\n",
    "H2 = 50\n",
    "H3 = 25\n",
    "D_out = 1\n",
    "\n",
    "print(D_in)\n",
    "#print(H)\n",
    "\n",
    "X_train4 = torch.from_numpy(X_train4)  #\n",
    "X_train4 = X_train4.float()            #\n",
    "y = torch.from_numpy(y_train4)         #\n",
    "y = y.float()\n",
    "y = y.view(y.shape[0],1)\n",
    "modelnnbcv4 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H3, D_out)\n",
    ")\n",
    "\n",
    "#model1.apply(weights_init)     # initialize weights\n",
    "\n",
    "loss_fn2 = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4    #orig: 1e-4\n",
    "\n",
    "#optimizer2 = torch.optim.SGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.Adagrad(modelnn.parameters(), lr= learning_rate, lr_decay= 0.1, weight_decay=0.3, initial_accumulator_value=0)#, eps= 1e-10)\n",
    "optimizer2= torch.optim.RMSprop(modelnnbcv4.parameters(), lr = learning_rate, alpha=0.85, eps=1e-08, weight_decay=0.06, momentum=0.9)\n",
    "#optimizer2 = torch.optim.Adamax(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, betas=(0.4, 0.999), eps=1e-6)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.ASGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, lambd=5e-1, alpha=0.65, t0=1e7)#, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "\n",
    "\n",
    "for t in range(10000):\n",
    "    optimizer2.zero_grad()\n",
    "    \n",
    "    y_pred = modelnnbcv4(X_train4)       ##\n",
    "    \n",
    "    loss = loss_fn2(y_pred, y)\n",
    "    if t%1000 == 0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer2.step()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "66330a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared Train: 0.9860265157902042\n",
      "R_squared val: 0.9529886346302084\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "X_train4 = torch.from_numpy(X_train4)        #\n",
    "X_train4 = X_train4.float()                 #\n",
    "Y_train_pred = modelnnbcv4(X_train4)           #\n",
    "Y_train_pred = Y_train_pred.detach().numpy()\n",
    "Y_train_pred = Y_train_pred.squeeze()\n",
    "Y_train = y_train4.squeeze()                #\n",
    "Y_mean = np.mean(Y_train)\n",
    "SS_tot = np.sum(np.power(Y_train - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_train_pred - Y_train, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared Train:', R_squared)\n",
    "R2_train_list.append(R_squared)\n",
    "\n",
    "\n",
    "\n",
    "X_test4 = torch.from_numpy(X_test4)      #\n",
    "X_test4 = X_test4.float()                #\n",
    "Y_test_pred = modelnnbcv4(X_test4)         #\n",
    "Y_test_pred = Y_test_pred.detach().numpy()\n",
    "Y_test_pred = Y_test_pred.squeeze()\n",
    "Y_test = y_test4.squeeze()            #\n",
    "Y_mean = np.mean(Y_test)\n",
    "SS_tot = np.sum(np.power(Y_test - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_test_pred - Y_test, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared val:', R_squared)\n",
    "R2_val_list.append(R_squared)\n",
    "\n",
    "\n",
    "# X_cos = torch.from_numpy(X_cos)\n",
    "# X_cos = X_cos.float()\n",
    "# Y_cos_pred = modelnnb(X_cos)\n",
    "# Y_cos_pred = Y_cos_pred.detach().numpy()\n",
    "# Y_cos_pred = Y_cos_pred.squeeze()\n",
    "# Y_cos = Y_cos.squeeze()\n",
    "# Y_mean = np.mean(Y_cos)\n",
    "# SS_tot = np.sum(np.power(Y_cos - Y_mean, 2))\n",
    "# SS_res = np.sum(np.power(Y_cos_pred - Y_cos, 2))\n",
    "# R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "# print('R_squared Cross Validation:', R_squared)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f1e27cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving Model\n",
    "\n",
    "# modelnnbcv4_pkl_filename = 'modelnnbcv4.pkl'\n",
    "# modelnnbcv4_pkl = open(modelnnbcv4_pkl_filename, 'wb')\n",
    "# pickle.dump(modelnnbcv4, modelnnbcv4_pkl)\n",
    "# modelnnbcv4_pkl.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bbe235",
   "metadata": {},
   "source": [
    "# Round 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "baad3724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2846, 15)\n",
      "(2847, 15) \n",
      "\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2846,)\n",
      "(2847,) \n",
      "\n",
      "----------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Neural Network\n",
    "\n",
    "y_true = segE_all[:,1]    # y_true= MD segregation energies\n",
    "\n",
    "pos = feature[:,:3]\n",
    "zpos = pos[:,2][:,np.newaxis]\n",
    "ypos= pos[:,1][:,np.newaxis]\n",
    "xpos= pos[:,0][:,np.newaxis]\n",
    "\n",
    "feature1 = feature[:,3:]\n",
    "\n",
    "# Separation of each descriptor (feature):\n",
    "f0= feature1[:,0][:,np.newaxis]\n",
    "f1= feature1[:,1][:,np.newaxis]\n",
    "f2= feature1[:,2][:,np.newaxis]\n",
    "f3= feature1[:,3][:,np.newaxis]\n",
    "f4= feature1[:,4][:,np.newaxis]\n",
    "f5= feature1[:,5][:,np.newaxis]\n",
    "f6= feature1[:,6][:,np.newaxis]\n",
    "f7= feature1[:,7][:,np.newaxis]\n",
    "f8= feature1[:,8][:,np.newaxis]\n",
    "f9= feature1[:,9][:,np.newaxis]\n",
    "f10= feature1[:,10][:,np.newaxis]\n",
    "f11= feature1[:,11][:,np.newaxis]\n",
    "f12= feature1[:,12][:,np.newaxis]\n",
    "f13= feature1[:,13][:,np.newaxis]\n",
    "f14= feature1[:,14][:,np.newaxis]\n",
    "f15= feature1[:,15][:,np.newaxis]\n",
    "f16= feature1[:,16][:,np.newaxis]\n",
    "f17= feature1[:,17][:,np.newaxis]\n",
    "\n",
    "# Removal of least important features below \n",
    "# The optimal combination of features to use was found using \n",
    "# Recursive Feature Elimination. This was applied\n",
    "# to each base estimator (see bottom of file for more info):\n",
    "feature1 = np.concatenate([f0,f1,f2,f3,f4,f8,f9,f10,f11,f13,f14,f15,f16,xpos,ypos], axis=1)\n",
    "\n",
    "n= feature1.shape[1]\n",
    "feature_space = feature1\n",
    "\n",
    "\n",
    "#random shuffle\n",
    "np.random.seed(10)   #Randomizing Seed\n",
    "idx0 = np.random.permutation(np.arange(len(feature_space)))  \n",
    "feature_space = feature_space[idx0]\n",
    "y_true = y_true[idx0]\n",
    "\n",
    "# #divide data into training, crossvalildation and test sets:\n",
    "# X_train = feature_space[0:int(len(feature_space)*.7)]\n",
    "# X_mean = np.mean(X_train, axis = 0)\n",
    "# X_std = np.std(X_train, axis = 0)\n",
    "# X_train = (X_train - X_mean)/X_std\n",
    "# X_temp = feature_space[int(len(feature_space)*.7):]\n",
    "# X_temp = (X_temp - X_mean)/X_std\n",
    "# X_test = X_temp[:int(len(X_temp)*.5)]\n",
    "# X_cos = X_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "# Y_train = y_true[0:int(len(feature_space)*.7)]\n",
    "# Y_mean = np.mean(Y_train, axis = 0)\n",
    "# Y_std = np.std(Y_train, axis = 0)\n",
    "# Y_temp = y_true[int(len(feature_space)*.7):]\n",
    "# Y_test = Y_temp[:int(len(X_temp)*.5)]\n",
    "# Y_cos = Y_temp[int(len(X_temp)*.5):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_fold1 = feature_space[0:int(len(feature_space)*.2)]\n",
    "X_fold2 = feature_space[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "X_fold3 = feature_space[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "X_fold4 = feature_space[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "X_fold5 = feature_space[int(len(feature_space)*.8):]\n",
    "\n",
    "X_std1 = np.std(X_fold1, axis = 0)\n",
    "X_std2 = np.std(X_fold2, axis = 0)\n",
    "X_std3 = np.std(X_fold3, axis = 0)\n",
    "X_std4 = np.std(X_fold4, axis = 0)\n",
    "X_std5 = np.std(X_fold5, axis = 0)\n",
    "\n",
    "X_mean1 = np.mean(X_fold1, axis = 0)\n",
    "X_mean2 = np.mean(X_fold2, axis = 0)\n",
    "X_mean3 = np.mean(X_fold3, axis = 0)\n",
    "X_mean4 = np.mean(X_fold4, axis = 0)\n",
    "X_mean5 = np.mean(X_fold5, axis = 0)\n",
    "\n",
    "X_fold1 = (X_fold1 - X_mean1)/X_std1\n",
    "X_fold2 = (X_fold2 - X_mean2)/X_std2\n",
    "X_fold3 = (X_fold3 - X_mean3)/X_std3\n",
    "X_fold4 = (X_fold4 - X_mean4)/X_std4\n",
    "X_fold5 = (X_fold5 - X_mean5)/X_std5\n",
    "\n",
    "\n",
    "y_fold1 = y_true[0:int(len(feature_space)*.2)]\n",
    "y_fold2 = y_true[int(len(feature_space)*.2):int(len(feature_space)*.4)]\n",
    "y_fold3 = y_true[int(len(feature_space)*.4):int(len(feature_space)*.6)]\n",
    "y_fold4 = y_true[int(len(feature_space)*.6):int(len(feature_space)*.8)]\n",
    "y_fold5 = y_true[int(len(feature_space)*.8):]\n",
    "\n",
    "\n",
    "\n",
    "print(X_fold1.shape)\n",
    "print(X_fold2.shape)\n",
    "print(X_fold3.shape)\n",
    "print(X_fold4.shape)\n",
    "print(X_fold5.shape,'\\n')\n",
    "\n",
    "print(y_fold1.shape)\n",
    "print(y_fold2.shape)\n",
    "print(y_fold3.shape)\n",
    "print(y_fold4.shape)\n",
    "print(y_fold5.shape, '\\n')\n",
    "\n",
    "\n",
    "\n",
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "print('-----------------------------','\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "392bcf77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "0 65.33096313476562\n",
      "1000 0.4826228618621826\n",
      "2000 0.41234856843948364\n",
      "3000 0.42075130343437195\n",
      "4000 0.36585333943367004\n",
      "5000 0.3703826069831848\n",
      "6000 0.3643879294395447\n",
      "7000 0.3803090751171112\n",
      "8000 0.3679853081703186\n",
      "9000 0.3438001275062561\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(10)\n",
    "\n",
    "\n",
    "(N, D_in) = X_train5.shape  #\n",
    "H1 = 200\n",
    "H2 = 50\n",
    "H3 = 25\n",
    "D_out = 1\n",
    "\n",
    "print(D_in)\n",
    "#print(H)\n",
    "\n",
    "X_train5 = torch.from_numpy(X_train5)  #\n",
    "X_train5 = X_train5.float()            #\n",
    "y = torch.from_numpy(y_train5)         #\n",
    "y = y.float()\n",
    "y = y.view(y.shape[0],1)\n",
    "modelnnbcv5 = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H1),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H1, H2),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H2, H3),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H3, D_out)\n",
    ")\n",
    "\n",
    "#model1.apply(weights_init)     # initialize weights\n",
    "\n",
    "loss_fn2 = torch.nn.MSELoss(reduction='sum')\n",
    "learning_rate = 1e-4    #orig: 1e-4\n",
    "\n",
    "#optimizer2 = torch.optim.SGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.Adagrad(modelnn.parameters(), lr= learning_rate, lr_decay= 0.1, weight_decay=0.3, initial_accumulator_value=0)#, eps= 1e-10)\n",
    "optimizer2= torch.optim.RMSprop(modelnnbcv5.parameters(), lr = learning_rate, alpha=0.85, eps=1e-08, weight_decay=0.06, momentum=0.9)\n",
    "#optimizer2 = torch.optim.Adamax(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, betas=(0.4, 0.999), eps=1e-6)#, weight_decay = 0.3)#0.2\n",
    "#optimizer2 = torch.optim.ASGD(modelnn.parameters(), lr = learning_rate, weight_decay=0.3, lambd=5e-1, alpha=0.65, t0=1e7)#, momentum = 0.9)#, weight_decay = 0.3)#0.2\n",
    "\n",
    "\n",
    "for t in range(10000):\n",
    "    optimizer2.zero_grad()\n",
    "    \n",
    "    y_pred = modelnnbcv5(X_train5)       ##\n",
    "    \n",
    "    loss = loss_fn2(y_pred, y)\n",
    "    if t%1000 == 0:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer2.step()\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "05e7293f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R_squared Train: 0.9843624145316054\n",
      "R_squared val: 0.9650318362239174\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n",
    "\n",
    "\n",
    "X_train5 = torch.from_numpy(X_train5)        #\n",
    "X_train5 = X_train5.float()                 #\n",
    "Y_train_pred = modelnnbcv5(X_train5)           #\n",
    "Y_train_pred = Y_train_pred.detach().numpy()\n",
    "Y_train_pred = Y_train_pred.squeeze()\n",
    "Y_train = y_train5.squeeze()                #\n",
    "Y_mean = np.mean(Y_train)\n",
    "SS_tot = np.sum(np.power(Y_train - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_train_pred - Y_train, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared Train:', R_squared)\n",
    "R2_train_list.append(R_squared)\n",
    "\n",
    "\n",
    "\n",
    "X_test5 = torch.from_numpy(X_test5)      #\n",
    "X_test5 = X_test5.float()                #\n",
    "Y_test_pred = modelnnbcv5(X_test5)         #\n",
    "Y_test_pred = Y_test_pred.detach().numpy()\n",
    "Y_test_pred = Y_test_pred.squeeze()\n",
    "Y_test = y_test5.squeeze()            #\n",
    "Y_mean = np.mean(Y_test)\n",
    "SS_tot = np.sum(np.power(Y_test - Y_mean, 2))\n",
    "SS_res = np.sum(np.power(Y_test_pred - Y_test, 2))\n",
    "R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "print('R_squared val:', R_squared)\n",
    "R2_val_list.append(R_squared)\n",
    "\n",
    "\n",
    "# X_cos = torch.from_numpy(X_cos)\n",
    "# X_cos = X_cos.float()\n",
    "# Y_cos_pred = modelnnb(X_cos)\n",
    "# Y_cos_pred = Y_cos_pred.detach().numpy()\n",
    "# Y_cos_pred = Y_cos_pred.squeeze()\n",
    "# Y_cos = Y_cos.squeeze()\n",
    "# Y_mean = np.mean(Y_cos)\n",
    "# SS_tot = np.sum(np.power(Y_cos - Y_mean, 2))\n",
    "# SS_res = np.sum(np.power(Y_cos_pred - Y_cos, 2))\n",
    "# R_squared = 1-SS_res/SS_tot\n",
    "\n",
    "# print('R_squared Cross Validation:', R_squared)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b3713bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving Model\n",
    "\n",
    "# modelnnbcv5_pkl_filename = 'modelnnbcv5.pkl'\n",
    "# modelnnbcv5_pkl = open(modelnnbcv5_pkl_filename, 'wb')\n",
    "# pickle.dump(modelnnbcv5, modelnnbcv5_pkl)\n",
    "# modelnnbcv5_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689d16b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db362d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "174db7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold4], axis=0)\n",
    "X_test1 = X_fold5\n",
    "y_train1 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold4], axis=0)\n",
    "y_test1 = y_fold5\n",
    "\n",
    "X_train2 = np.concatenate([X_fold1,X_fold2,X_fold3,X_fold5], axis=0)\n",
    "X_test2= X_fold4\n",
    "y_train2 = np.concatenate([y_fold1,y_fold2,y_fold3,y_fold5], axis=0)\n",
    "y_test2 = y_fold4\n",
    "\n",
    "X_train3 = np.concatenate([X_fold1,X_fold2,X_fold4,X_fold5], axis=0)\n",
    "X_test3 = X_fold3\n",
    "y_train3 = np.concatenate([y_fold1,y_fold2,y_fold4,y_fold5], axis=0)\n",
    "y_test3 = y_fold3\n",
    "\n",
    "X_train4 = np.concatenate([X_fold1,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test4 = X_fold2\n",
    "y_train4 = np.concatenate([y_fold1,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test4 = y_fold2\n",
    "\n",
    "X_train5 = np.concatenate([X_fold2,X_fold3,X_fold4,X_fold5], axis=0)\n",
    "X_test5 = X_fold1\n",
    "y_train5 = np.concatenate([y_fold2,y_fold3,y_fold4,y_fold5], axis=0)\n",
    "y_test5 = y_fold1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b72c352d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.005582242943323989 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation 1\n",
    "\n",
    "X_test1 = torch.from_numpy(X_test1)\n",
    "X_test1 = X_test1.float()\n",
    "Y_test_pred_nn = modelnnbcv2(X_test1)\n",
    "Y_test_pred_nn = Y_test_pred_nn.detach().numpy()\n",
    "Y_test_pred_nn = Y_test_pred_nn.squeeze()\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test1,Y_test_pred_nn)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fac2c98b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.008675287690438323 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation 2\n",
    "\n",
    "X_test2 = torch.from_numpy(X_test2)\n",
    "X_test2 = X_test2.float()\n",
    "Y_test_pred_nn = modelnnbcv2(X_test2)\n",
    "Y_test_pred_nn = Y_test_pred_nn.detach().numpy()\n",
    "Y_test_pred_nn = Y_test_pred_nn.squeeze()\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test2,Y_test_pred_nn)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d80d865f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.008931011390962097 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation 3\n",
    "\n",
    "X_test3 = torch.from_numpy(X_test3)\n",
    "X_test3 = X_test3.float()\n",
    "Y_test_pred_nn = modelnnbcv3(X_test3)\n",
    "Y_test_pred_nn = Y_test_pred_nn.detach().numpy()\n",
    "Y_test_pred_nn = Y_test_pred_nn.squeeze()\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test3,Y_test_pred_nn)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ad36f3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.009561601200919543 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation 4\n",
    "\n",
    "X_test4 = torch.from_numpy(X_test4)\n",
    "X_test4 = X_test4.float()\n",
    "Y_test_pred_nn = modelnnbcv4(X_test4)\n",
    "Y_test_pred_nn = Y_test_pred_nn.detach().numpy()\n",
    "Y_test_pred_nn = Y_test_pred_nn.squeeze()\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test4,Y_test_pred_nn)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39855321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.00862574280501505 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RMSE Calculation 5\n",
    "\n",
    "X_test5 = torch.from_numpy(X_test5)\n",
    "X_test5 = X_test5.float()\n",
    "Y_test_pred_nn = modelnnbcv5(X_test5)\n",
    "Y_test_pred_nn = Y_test_pred_nn.detach().numpy()\n",
    "Y_test_pred_nn = Y_test_pred_nn.squeeze()\n",
    "\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_test5,Y_test_pred_nn)),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4db4bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0013870749622810078\n"
     ]
    }
   ],
   "source": [
    "rmse = [0.005582242943323989,0.008675287690438323,0.008931011390962097 , 0.009561601200919543 , 0.00862574280501505 ]\n",
    "print(np.std(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d102d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9849871183215958, 0.9855009500880881, 0.9834642996058409, 0.9860265157902042, 0.9843624145316054]\n",
      "[0.9643650124224278, 0.9621413513406336, 0.960232123313809, 0.9529886346302084, 0.9650318362239174]\n"
     ]
    }
   ],
   "source": [
    "print(R2_train_list)\n",
    "print(R2_val_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "67e54708",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000892520582393606\n",
      "0.004327624851117232\n"
     ]
    }
   ],
   "source": [
    "print(np.std(R2_train_list))\n",
    "print(np.std(R2_val_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee40203c",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bb66f97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R_squared_train_list = [0.9872147429362317, 0.9888766333675835, 0.9871230941543926, 0.9893489991094322, 0.9893383223780539]\n",
    "# R_squared_val_list = [0.9626601024742505, 0.965477491545618, 0.9616423575655719, 0.9593789281882419, 0.9674682981144371]\n",
    "# rmse_list = [0.009222660053090272,  0.008284236793446318, 0.008284236793446318, 0.00888801712687602, 0.008319811424252305]\n",
    "\n",
    "# print(R_squared_train_list)\n",
    "# print(R_squared_val_list)\n",
    "# print(rmse_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6b587fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.std(R_squared_train_list))\n",
    "# print(np.std(R_squared_val_list))\n",
    "# print(np.std(rmse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d58546f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
